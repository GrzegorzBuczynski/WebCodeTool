"""
Modu≈Ç agent√≥w AI - r√≥≈ºne typy agent√≥w do dekompozycji, wykonania i weryfikacji zada≈Ñ
"""
import os
import time
from typing import List, Dict, Any, Optional
from openai import OpenAI
from .task_manager import Task, TaskStatus, TaskType, TaskManager
from .persistence import PersistenceManager
from colorama import Fore, Style, init

init(autoreset=True)


class BaseAgent:
    """Bazowa klasa dla wszystkich agent√≥w"""
    
    def __init__(self, name: str, role: str, api_key: Optional[str] = None, 
                 provider: Optional[str] = None, model: Optional[str] = None):
        self.name = name
        self.role = role
        self.provider = provider or os.getenv("AI_PROVIDER", "openai")
        self.model = model or os.getenv("MODEL", "gpt-4o-mini")
        
        # Konfiguracja klienta w zale≈ºno≈õci od providera
        if self.provider == "openai":
            self.client = OpenAI(api_key=api_key or os.getenv("API_KEY"))
        elif self.provider == "openrouter":
            self.client = OpenAI(
                api_key=api_key or os.getenv("API_KEY"),
                base_url="https://openrouter.ai/api/v1"
            )
        elif self.provider == "ollama":
            self.client = OpenAI(
                api_key="ollama",  # Ollama nie wymaga klucza
                base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1")
            )
        else:
            raise ValueError(f"Nieobs≈Çugiwany dostawca API: {self.provider}")
        
    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Wywo≈Çuje model jƒôzykowy"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"{Fore.RED}B≈ÇƒÖd wywo≈Çania LLM: {e}")
            return ""
    
    def log(self, message: str, color=Fore.WHITE):
        """Loguje wiadomo≈õƒá z kolorami"""
        print(f"{color}[{self.name}] {message}{Style.RESET_ALL}")


class ComplexityAnalyzerAgent(BaseAgent):
    """Agent analizujƒÖcy z≈Ço≈ºono≈õƒá - ocenia czy zadanie wymaga podzia≈Çu"""
    
    def __init__(self, api_key: Optional[str] = None, provider: Optional[str] = None,
                 model: Optional[str] = None):
        super().__init__("ComplexityAnalyzer", "Complexity Assessment", api_key, provider, model)
    
    def should_decompose(self, task: Task) -> Dict[str, Any]:
        """Ocenia czy zadanie wymaga podzia≈Çu na podzadania"""
        self.log(f"Analizujƒô: {task.description[:50]}...", Fore.MAGENTA)
        
        system_prompt = """Jeste≈õ ekspertem w analizie z≈Ço≈ºono≈õci zada≈Ñ. OCENIASZ POTENCJALNY OUTPUT!
Oceniasz zadania pod kƒÖtem:
1. POTENCJALNEJ ILO≈öCI OUTPUTU - ile tekstu/danych wygeneruje to zadanie?
2. Czy zadanie jest wystarczajƒÖco PROSTE do bezpo≈õredniego wykonania
3. Liczba krok√≥w wymaganych do wykonania

KRYTERIA POTENCJALNEGO OUTPUTU (to jest KLUCZOWE!):
KR√ìTKI (< 500 s≈Ç√≥w): Prosta odpowied≈∫, kilka zda≈Ñ, lista
≈öREDNI (500-1500 s≈Ç√≥w): Kr√≥tkie wyja≈õnienie, kilka akapit√≥w
D≈ÅUGI (1500-5000 s≈Ç√≥w): Raport, wiele sekcji, szczeg√≥≈Çowe om√≥wienie
BARDZO_D≈ÅUGI (> 5000 s≈Ç√≥w): Bardzo szczeg√≥≈Çowy raport, analiza wieloaspektowa

Kryteria PROSTEGO zadania (nie wymaga podzia≈Çu):
- Potencjalny output: KR√ìTKI lub ≈öREDNI
- Mo≈ºna wykonaƒá w jednym kroku
- Nie wymaga wielu r√≥≈ºnych analiz/oblicze≈Ñ
- Jest konkretne i jednoznaczne

Kryteria Z≈ÅO≈ªONEGO zadania (wymaga podzia≈Çu):
- Potencjalny output: D≈ÅUGI lub BARDZO_D≈ÅUGI
- Wymaga wielu krok√≥w lub analiz
- Obejmuje r√≥≈ºne aspekty/dziedziny
- Zbyt szerokie lub wielowƒÖtkowe

Odpowiedz w formacie:
POTENCJALNY_OUTPUT: [KR√ìTKI/≈öREDNI/D≈ÅUGI/BARDZO_D≈ÅUGI]
PODZIA≈Å: [TAK/NIE]
LICZBA_PODZADA≈É: [2-5 je≈õli TAK, 0 je≈õli NIE]
Z≈ÅO≈ªONO≈öƒÜ: [NISKA/≈öREDNIA/WYSOKA/BARDZO_WYSOKA]
UZASADNIENIE: [wyja≈õnienie potencjalnego outputu i decyzji]"""

        user_prompt = f"""Zadanie do oceny:
{task.description}

Aktualny poziom zagnie≈ºd≈ºenia: {task.level}

SKUPIAJ SIƒò NA POTENCJALNYM OUTPUTIE - ile tekstu/danych wygeneruje to zadanie?
Czy to zadanie wymaga podzia≈Çu na podzadania?"""

        response = self._call_llm(system_prompt, user_prompt)
        
        # Parsuj odpowied≈∫
        analysis = self._parse_complexity_response(response)
        
        if analysis["should_split"]:
            self.log(f"‚úì Zadanie WYMAGA podzia≈Çu na {analysis['num_subtasks']} podzada≈Ñ", Fore.YELLOW)
            self.log(f"  Output: {analysis['output_size']} | Z≈Ço≈ºono≈õƒá: {analysis['complexity']}", Fore.CYAN)
        else:
            self.log(f"‚úì Zadanie jest WYSTARCZAJƒÑCO PROSTE - wykonaj bezpo≈õrednio", Fore.GREEN)
            self.log(f"  Output: {analysis['output_size']} | Z≈Ço≈ºono≈õƒá: {analysis['complexity']}", Fore.CYAN)
        
        return analysis
    
    def _parse_complexity_response(self, response: str) -> Dict[str, Any]:
        """Parsuje odpowied≈∫ o z≈Ço≈ºono≈õci"""
        import re
        
        analysis = {
            "should_split": False,
            "num_subtasks": 0,
            "complexity": "≈öREDNIA",
            "output_size": "≈öREDNI",
            "reasoning": ""
        }
        
        lines = response.upper().split('\n')
        for line in lines:
            if 'POTENCJALNY_OUTPUT' in line or 'POTENCJALNY OUTPUT' in line:
                if 'BARDZO_D\u0141UGI' in line or 'BARDZO D\u0141UGI' in line or 'BARDZO_DLUGI' in line or 'BARDZO DLUGI' in line:
                    analysis["output_size"] = "BARDZO_D\u0141UGI"
                elif 'D\u0141UGI' in line or 'DLUGI' in line:
                    analysis["output_size"] = "D\u0141UGI"
                elif '\u015aREDNI' in line or 'SREDNI' in line:
                    analysis["output_size"] = "\u015aREDNI"
                elif 'KR\u00d3TKI' in line or 'KROTKI' in line:
                    analysis["output_size"] = "KR\u00d3TKI"
            elif 'PODZIA\u0141:' in line or 'PODZIAL' in line:
                analysis["should_split"] = 'TAK' in line
            elif 'LICZBA_PODZADA\u0143' in line or 'LICZBA PODZADAN' in line or 'LICZBA_PODZADAN' in line:
                numbers = re.findall(r'\d+', line)
                if numbers:
                    analysis["num_subtasks"] = min(int(numbers[0]), 5)
            elif 'Z\u0141O\u017bONO\u015a\u0106:' in line or 'ZLOZONOSC' in line:
                if 'BARDZO_WYSOKA' in line or 'BARDZO WYSOKA' in line or 'BARDZO_WYSOKA' in line:
                    analysis["complexity"] = "BARDZO_WYSOKA"
                elif 'WYSOKA' in line:
                    analysis["complexity"] = "WYSOKA"
                elif '\u015aREDNIA' in line or 'SREDNIA' in line:
                    analysis["complexity"] = "\u015aREDNIA"
                elif 'NISKA' in line:
                    analysis["complexity"] = "NISKA"
            elif 'UZASADNIENIE:' in line:
                analysis["reasoning"] = line.split(':', 1)[1].strip()
        
        # Je\u015bli num_subtasks to 0 ale should_split to True, ustaw domy\u015blnie 3
        if analysis["should_split"] and analysis["num_subtasks"] == 0:
            analysis["num_subtasks"] = 3
        
        return analysis


class CoordinatorAgent(BaseAgent):
    """Agent koordynujƒÖcy - analizuje cel i dzieli na podzadania"""
    
    def __init__(self, api_key: Optional[str] = None, provider: Optional[str] = None, 
                 model: Optional[str] = None):
        super().__init__("Coordinator", "Task Decomposition", api_key, provider, model)
        
    def decompose_task(self, task: Task, max_subtasks: int, task_manager=None) -> List[str]:
        """Dekomponuje zadanie na podzadania"""
        self.log(f"Analizujƒô zadanie: {task.description}", Fore.CYAN)
        
        # Je≈õli max_subtasks = 1, zadanie jest atomowe
        if max_subtasks == 1:
            self.log("Zadanie ocenione jako atomowe - nie wymaga dekompozycji", Fore.YELLOW)
            return []
        
        system_prompt = f"""Jeste≈õ ekspertem w dekompozycji zada≈Ñ. Twoim zadaniem jest roz≈Ço≈ºenie z≈Ço≈ºonego 
zadania na dok≈Çadnie {max_subtasks} mniejszych, wykonalnych podzada≈Ñ.

Zasady:
1. Ka≈ºde podzadanie powinno byƒá konkretne i wykonalne
2. Podzadania powinny byƒá logicznie uporzƒÖdkowane
3. Razem podzadania powinny w pe≈Çni realizowaƒá g≈Ç√≥wne zadanie
4. Zwr√≥ƒá DOK≈ÅADNIE {max_subtasks} podzada≈Ñ, ka≈ºde w osobnej linii, numerowane
5. Nie zwracaj wiƒôcej ani mniej ni≈º {max_subtasks} podzada≈Ñ"""

        parent_context = ""
        if task.parent_id and task_manager:
            parent_task = task_manager.get_task(task.parent_id)
            if parent_task:
                parent_context = f"\nKontekst z zadania nadrzƒôdnego: {parent_task.description}"

        user_prompt = f"""G≈Ç√≥wne zadanie (poziom {task.level}):
{task.description}
{parent_context}

Roz≈Ç√≥≈º to zadanie na DOK≈ÅADNIE {max_subtasks} podzada≈Ñ."""

        response = self._call_llm(system_prompt, user_prompt)
        
        # Parsuj odpowied≈∫
        subtasks = []
        for line in response.strip().split('\n'):
            line = line.strip()
            if line and (line[0].isdigit() or line.startswith('-') or line.startswith('‚Ä¢')):
                # Usu≈Ñ numeracjƒô i bia≈Çe znaki
                clean_line = line.lstrip('0123456789.-‚Ä¢) ').strip()
                if clean_line:
                    subtasks.append(clean_line)
        
        # Upewnij siƒô, ≈ºe mamy odpowiedniƒÖ liczbƒô
        if len(subtasks) > max_subtasks:
            subtasks = subtasks[:max_subtasks]
        
        self.log(f"Utworzono {len(subtasks)} podzada≈Ñ", Fore.GREEN)
        return subtasks


class ExecutorAgent(BaseAgent):
    """Agent wykonawczy - realizuje atomowe zadania"""
    
    def __init__(self, agent_id: int, api_key: Optional[str] = None, 
                 provider: Optional[str] = None, model: Optional[str] = None):
        super().__init__(f"Executor-{agent_id}", "Task Execution", api_key, provider, model)
        self.agent_id = agent_id
        
    def execute_task(self, task: Task, context: Dict[str, Any] = None) -> str:
        """Wykonuje zadanie i zwraca wynik"""
        self.log(f"Wykonujƒô zadanie: {task.description[:50]}...", Fore.BLUE)
        
        context_info = ""
        if context:
            context_info = f"\nKontekst z poprzednich zada≈Ñ:\n{self._format_context(context)}"
        
        system_prompt = """Jeste≈õ specjalistycznym agentem wykonawczym. Twoim zadaniem jest wykonanie 
konkretnego zadania i przedstawienie szczeg√≥≈Çowego wyniku.

Zasady:
1. Wykonaj zadanie dok≈Çadnie wed≈Çug opisu
2. Zwr√≥ƒá konkretny, mierzalny wynik
3. Je≈õli potrzebujesz danych z kontekstu, wykorzystaj je
4. Wynik powinien byƒá zwiƒôz≈Çy ale kompletny
5. Je≈õli zadanie nie mo≈ºe byƒá wykonane, wyja≈õnij dlaczego"""

        user_prompt = f"""Zadanie do wykonania:
{task.description}
{context_info}

Wykonaj zadanie i przedstaw wynik."""

        result = self._call_llm(system_prompt, user_prompt)
        self.log("Zadanie uko≈Ñczone", Fore.GREEN)
        
        return result
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """Formatuje kontekst dla LLM"""
        formatted = []
        for key, value in context.items():
            formatted.append(f"- {key}: {value}")
        return "\n".join(formatted)


class VerificationAgent(BaseAgent):
    """Agent weryfikujƒÖcy - sprawdza jako≈õƒá wykonania zada≈Ñ"""
    
    def __init__(self, api_key: Optional[str] = None, provider: Optional[str] = None,
                 model: Optional[str] = None):
        super().__init__("Verifier", "Quality Assurance", api_key, provider, model)
        
    def verify_task(self, task: Task) -> Dict[str, Any]:
        """Weryfikuje wykonanie zadania"""
        self.log(f"Weryfikujƒô zadanie: {task.description[:50]}...", Fore.MAGENTA)
        
        if not task.result:
            return {
                "passed": False,
                "score": 0.0,
                "feedback": "Brak wyniku do weryfikacji",
                "issues": ["Zadanie nie zosta≈Ço wykonane"]
            }
        
        # === VALUE-ADDED FILTER (Heurystyka Ciƒôcia nr 3) ===
        # Sprawd≈∫ czy wynik jest "pusty" (np. tylko instrukcje)
        value_added_check = self._check_value_added(task)
        
        if not value_added_check["has_value"]:
            self.log(f"‚úó BRAK WARTO≈öCI: Wynik jest pusty - tylko instrukcje bez tre≈õci", Fore.RED)
            return {
                "passed": False,
                "score": 0.0,
                "feedback": f"Wynik nie ma warto≈õci dodanej. {value_added_check['reason']}",
                "issues": ["Wynik zawiera tylko instrukcje, brakuje rzeczywistych danych/analizy/kodu"]
            }
        
        system_prompt = """Jeste≈õ ekspertem w kontroli jako≈õci i weryfikacji zada≈Ñ.
Twoim zadaniem jest ocena czy zadanie zosta≈Ço wykonane poprawnie i kompletnie.

KRYTERIA WARTO≈öCI DODANEJ (Value-Added):
‚úì AKCEPTOWALNE wyniki zawierajƒÖ co najmniej jedno z:
  - Analiza (insights, wnioski, interpretacja)
  - Tekst (opisowe wyja≈õnienia, szczeg√≥≈Çowe om√≥wienia)
  - Kod (skrypty, funkcje, implementacja)
  - Tabela/Dane (strukturyzowane dane, metryki)

‚úó NIEAKCEPTOWALNE wyniki zawierajƒÖ TYLKO:
  - "Szukaj tu..." (instrukcje bez tre≈õci)
  - "Przeczytaj plik..." (linki bez analizy)
  - "U≈ºyj API..." (wskaz√≥wki bez implementacji)
  - "Sprawd≈∫ dokumentacjƒô..." (referencje bez kontekstu)

Zwr√≥ƒá odpowied≈∫ w formacie:
OCENA: [PASS/FAIL]
PUNKTACJA: [0.0-10.0]
FEEDBACK: [Szczeg√≥≈Çowa ocena]
PROBLEMY: [Lista problem√≥w lub "Brak"]"""

        user_prompt = f"""Zadanie:
{task.description}

Wynik wykonania:
{task.result}

Oce≈Ñ jako≈õƒá wykonania zadania. Sprawd≈∫ czy zawiera RZECZYWISTƒÑ WARTO≈öƒÜ (analizƒô, tekst, kod, dane)."""

        response = self._call_llm(system_prompt, user_prompt)
        
        # Parsuj odpowied≈∫
        verification = self._parse_verification(response)
        
        if verification["passed"]:
            self.log(f"‚úì Weryfikacja zako≈Ñczona sukcesem (wynik: {verification['score']}/10)", Fore.GREEN)
        else:
            self.log(f"‚úó Weryfikacja nie powiod≈Ça siƒô (wynik: {verification['score']}/10)", Fore.RED)
        
        return verification
    
    def _check_value_added(self, task: Task) -> Dict[str, Any]:
        """Sprawdza czy wynik ma warto≈õƒá dodanƒÖ (nie jest pusty/instrukcjƒÖ)"""
        result = task.result.lower() if task.result else ""
        
        # Wska≈∫niki "pustych" wynik√≥w
        empty_indicators = [
            "szukaj",
            "wyszukaj",
            "sprawd≈∫",
            "przeczytaj",
            "przejd≈∫ do",
            "odwied≈∫",
            "kliknij",
            "u≈ºyj api",
            "u≈ºyj biblioteki",
            "skontaktuj siƒô",
            "zapytaj",
            "jaki jest",
            "jak znale≈∫ƒá",
            "gdzie znale≈∫ƒá",
            "instrukcja:",
            "przewodnik:",
            "linki do",
            "referencje do",
            "zobacz dokumentacjƒô",
            "dokumentacja",
            "handbook",
        ]
        
        # Wska≈∫niki "pe≈Çnych" wynik√≥w
        value_indicators = [
            "analiza",
            "wynik",
            "dane",
            "statystyka",
            "wykazuje",
            "pokazuje",
            "na podstawie",
            "wyliczenie",
            "zaproponowaƒá",
            "rekomendacja",
            "kod",
            "import",
            "tabela",
            "tablica",
            "liczba:",
            "warto≈õƒá:",
            "procent",
            "%",
            "ponad 80%",
        ]
        
        empty_count = sum(1 for indicator in empty_indicators if indicator in result)
        value_count = sum(1 for indicator in value_indicators if indicator in result)
        
        # Je≈õli pusty wska≈∫nik pojawia siƒô 3+ razy i nie ma warto≈õci, to fail
        if empty_count >= 3 and value_count < 2:
            return {
                "has_value": False,
                "reason": "Wynik zawiera g≈Ç√≥wnie instrukcje ('szukaj', 'sprawd≈∫', etc.) bez rzeczywistych danych."
            }
        
        # Je≈õli wynik jest za kr√≥tki (< 50 znak√≥w), to nie ma warto≈õci
        if len(result) < 50:
            return {
                "has_value": False,
                "reason": "Wynik jest zbyt kr√≥tki - przypomina instrukcjƒô, a nie kompletne rozwiƒÖzanie."
            }
        
        # Je≈õli wynik zawiera warto≈õciowe wska≈∫niki, to OK
        if value_count >= 1:
            return {"has_value": True, "reason": ""}
        
        # Domy≈õlnie akceptuj (LLM zrobi ostatecznƒÖ ocenƒô)
        return {"has_value": True, "reason": ""}
    
    def _parse_verification(self, response: str) -> Dict[str, Any]:
        """Parsuje odpowied≈∫ weryfikacyjnƒÖ"""
        lines = response.strip().split('\n')
        verification = {
            "passed": False,
            "score": 0.0,
            "feedback": "",
            "issues": []
        }
        
        for line in lines:
            line = line.strip()
            if line.startswith("OCENA:"):
                verification["passed"] = "PASS" in line.upper()
            elif line.startswith("PUNKTACJA:"):
                try:
                    score_str = line.split(':')[1].strip()
                    verification["score"] = float(score_str.split('/')[0])
                except:
                    verification["score"] = 5.0
            elif line.startswith("FEEDBACK:"):
                verification["feedback"] = line.split(':', 1)[1].strip()
            elif line.startswith("PROBLEMY:"):
                issues_str = line.split(':', 1)[1].strip()
                if issues_str.lower() != "brak":
                    verification["issues"] = [issues_str]
        
        return verification


class SemanticLoopDetectorAgent(BaseAgent):
    """Agent wykrywajƒÖcy pƒôtle biurokratyczne - gdy zadanie na poziomie 4 == zadaniu na poziomie 1"""
    
    def __init__(self, api_key: Optional[str] = None, provider: Optional[str] = None,
                 model: Optional[str] = None):
        super().__init__("SemanticLoopDetector", "Loop Detection", api_key, provider, model)
    
    def detect_semantic_loops(self, task: Task, task_manager: TaskManager) -> Dict[str, Any]:
        """Wykrywa semantyczne pƒôtle - zadanie na g≈Çƒôbokim poziomie identyczne ze swoim przodkiem"""
        
        # Zbierz wszystkich przodk√≥w
        ancestors = []
        current = task
        while current.parent_id:
            parent = task_manager.get_task(current.parent_id)
            if parent:
                ancestors.append(parent)
                current = parent
            else:
                break
        
        if not ancestors:
            return {"loop_detected": False, "ancestor_match": None}
        
        self.log(f"Sprawdzam {task.description[:40]}... wzglƒôdem {len(ancestors)} przodk√≥w", Fore.MAGENTA)
        
        # Por√≥wnaj z ka≈ºdym przodkiem
        for ancestor in ancestors:
            system_prompt = """Jeste≈õ ekspertem w analizie semantycznej. 
Twoim zadaniem jest sprawdziƒá czy dwa zadania sƒÖ semantycznie identyczne lub prawie identyczne.

Por√≥wnaj dwa zadania:
1. Czy robiƒÖ dok≈Çadnie to samo?
2. Czy jedno jest podzia≈Çem drugiego (co sugerowa≈Çoby pƒôtlƒô)?
3. Czy sƒÖ komplementarne czy redundantne?

Odpowiedz w formacie:
IDENTYCZNE: [TAK/NIE]
STOPIE≈É_PODOBIE≈ÉSTWA: [0-100]
PƒòTLA: [TAK/NIE]
UZASADNIENIE: [kr√≥tkie wyja≈õnienie]"""

            user_prompt = f"""Zadanie nadrzƒôdne (Level {ancestor.level}):
{ancestor.description}

Zadanie potomne (Level {task.level}):
{task.description}

Sprawd≈∫ czy sƒÖ semantycznie identyczne lub czy zadanie potomne wciela siƒô w pƒôtlƒô do przodka."""

            response = self._call_llm(system_prompt, user_prompt)
            
            # Parsuj odpowied≈∫
            is_loop = "PƒòTLA: TAK" in response.upper()
            similarity = 0
            try:
                for line in response.split('\n'):
                    if 'STOPIE≈É_PODOBIE≈ÉSTWA' in line or 'STOPIEN_PODOBIENSTWA' in line:
                        import re
                        nums = re.findall(r'\d+', line)
                        if nums:
                            similarity = int(nums[0])
                        break
            except:
                pass
            
            if is_loop or similarity > 85:
                self.log(f"üîÑ PƒòTLA WYKRYTA: '{task.description[:40]}...' powtarza '{ancestor.description[:40]}...'", Fore.RED)
                return {
                    "loop_detected": True,
                    "ancestor_match": ancestor,
                    "similarity": similarity,
                    "analysis": response
                }
        
        self.log(f"‚úì Brak pƒôtli semantycznej", Fore.GREEN)
        return {"loop_detected": False, "ancestor_match": None}


class DuplicationDetectorAgent(BaseAgent):
    """Agent wykrywajƒÖcy i eliminujƒÖcy pokrywajƒÖce siƒô zadania"""
    
    def __init__(self, api_key: Optional[str] = None, provider: Optional[str] = None,
                 model: Optional[str] = None):
        super().__init__("DuplicationDetector", "Duplication Analysis", api_key, provider, model)
    
    def detect_and_eliminate_duplicates(self, subtask_descriptions: List[str], 
                                       parent_task: Task) -> List[str]:
        """Wykrywa i eliminuje pokrywajƒÖce siƒô zadania"""
        if len(subtask_descriptions) <= 1:
            return subtask_descriptions
        
        self.log(f"Analizujƒô {len(subtask_descriptions)} podzada≈Ñ pod kƒÖtem duplikat√≥w", Fore.MAGENTA)
        
        system_prompt = """Jeste≈õ ekspertem w analizie zada≈Ñ i wykrywaniu duplikat√≥w.
Twoim zadaniem jest przeanalizowaƒá listƒô podzada≈Ñ i:
1. Zidentyfikowaƒá zadania, kt√≥re siƒô pokrywajƒÖ lub sƒÖ duplikatami
2. Wybraƒá najlepsze, najbardziej kompletne wersje zada≈Ñ
3. Zwr√≥ciƒá TYLKO unikalne, niepokrywajƒÖce siƒô zadania

Zasady eliminacji:
- Je≈õli 2 zadania robiƒÖ to samo, zostaw JEDNO (bardziej kompletne)
- Je≈õli zadanie A zawiera siƒô w zadaniu B, zostaw TYLKO B
- Je≈õli zadania sƒÖ komplementarne (r√≥≈ºne aspekty), ZOSTAW OBA
- Zwr√≥ƒá TYLKO listƒô unikalnych zada≈Ñ, ka≈ºde w nowej linii, bez numeracji
- Je≈õli wszystkie zadania sƒÖ unikalne, zwr√≥ƒá wszystkie"""

        tasks_list = "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(subtask_descriptions)])
        
        user_prompt = f"""Zadanie nadrzƒôdne:
{parent_task.description}

Lista podzada≈Ñ do analizy:
{tasks_list}

Przeanalizuj te podzadania i zwr√≥ƒá TYLKO unikalne, niepokrywajƒÖce siƒô zadania (bez numeracji)."""

        response = self._call_llm(system_prompt, user_prompt)
        
        # Parsuj odpowied≈∫
        unique_tasks = []
        for line in response.strip().split('\n'):
            line = line.strip()
            if line and not line.startswith('#'):
                # Usu≈Ñ ewentualnƒÖ numeracjƒô
                clean_line = line.lstrip('0123456789.-‚Ä¢) ').strip()
                if clean_line and len(clean_line) > 10:  # Minimum 10 znak√≥w dla sensownego zadania
                    unique_tasks.append(clean_line)
        
        eliminated_count = len(subtask_descriptions) - len(unique_tasks)
        
        if eliminated_count > 0:
            self.log(f"Wyeliminowano {eliminated_count} pokrywajƒÖcych siƒô zada≈Ñ", Fore.YELLOW)
            self.log(f"Pozosta≈Ço {len(unique_tasks)} unikalnych zada≈Ñ", Fore.GREEN)
        else:
            self.log(f"Wszystkie {len(unique_tasks)} zadania sƒÖ unikalne", Fore.GREEN)
        
        return unique_tasks if unique_tasks else subtask_descriptions


class MasterOrchestrator:
    """G≈Ç√≥wny orkiestrator zarzƒÖdzajƒÖcy wszystkimi agentami"""
    
    def __init__(self, task_manager: TaskManager, api_key: Optional[str] = None,
                 provider: Optional[str] = None, model: Optional[str] = None,
                 max_recursion_depth: int = 10, persistence_dir: str = "results"):
        self.task_manager = task_manager
        self.max_recursion_depth = max_recursion_depth  # Safety limit przeciw niesko≈Ñczonej rekursji
        self.provider = provider or os.getenv("AI_PROVIDER", "openai")
        self.model = model or os.getenv("MODEL", "gpt-4o-mini")
        self.persistence = PersistenceManager(persistence_dir)
        self.complexity_analyzer = ComplexityAnalyzerAgent(api_key, provider, model)
        self.coordinator = CoordinatorAgent(api_key, provider, model)
        self.duplication_detector = DuplicationDetectorAgent(api_key, provider, model)
        self.semantic_loop_detector = SemanticLoopDetectorAgent(api_key, provider, model)
        self.verifier = VerificationAgent(api_key, provider, model)
        self.executors = [ExecutorAgent(i, api_key, provider, model) for i in range(1, 6)]
        self.executor_index = 0
        self.context_store: Dict[str, Any] = {}
        self.decomposition_stats = {
            "total_tasks": 0,
            "decomposed": 0,
            "executed_directly": 0,
            "max_level_reached": 0
        }
        self.execution_start_time = time.time()
        
    def log(self, message: str, color=Fore.WHITE):
        """Loguje wiadomo≈õƒá"""
        print(f"{color}[Orchestrator] {message}{Style.RESET_ALL}")
    
    def get_next_executor(self) -> ExecutorAgent:
        """Pobiera nastƒôpnego dostƒôpnego executora (round-robin)"""
        executor = self.executors[self.executor_index]
        self.executor_index = (self.executor_index + 1) % len(self.executors)
        return executor
    
    def process_task_recursive(self, task: Task) -> bool:
        """Rekursywnie przetwarza zadanie z inteligentnƒÖ ocenƒÖ potrzeby podzia≈Çu"""
        self.log(f"\n{'='*80}\nPoziom {task.level}: Przetwarzanie zadania {task.id}", Fore.YELLOW)
        print(f"Opis: {task.description}\n{'='*80}")
        
        self.decomposition_stats["total_tasks"] += 1
        self.decomposition_stats["max_level_reached"] = max(
            self.decomposition_stats["max_level_reached"], 
            task.level
        )
        
        # === MAX DEPTH GUARD (Heurystyka Ciƒôcia nr 1) ===
        # Je≈õli poziom > 3, natychmiast przejd≈∫ do Direct Completion
        if task.level > 3:
            self.log(f"‚ö† LIMIT G≈ÅƒòBOKO≈öCI: Zadanie na poziomie {task.level} > 3 -> wymuszam bezpo≈õrednie wykonanie", Fore.RED)
            self.decomposition_stats["executed_directly"] += 1
            return self._execute_atomic_task(task)
        
        # Safety limit - ochrona przed niesko≈ÑczonƒÖ rekursjƒÖ
        if task.level >= self.max_recursion_depth:
            self.log(f"‚ö† UWAGA: OsiƒÖgniƒôto limit bezpiecze≈Ñstwa ({self.max_recursion_depth}) - wymuszam wykonanie", Fore.RED)
            self.decomposition_stats["executed_directly"] += 1
            return self._execute_atomic_task(task)
        
        # Krok 1: Complexity Analyzer ocenia czy zadanie wymaga podzia≈Çu
        complexity_analysis = self.complexity_analyzer.should_decompose(task)
        
        # Je≈õli zadanie jest wystarczajƒÖco proste, wykonaj bezpo≈õrednio
        if not complexity_analysis["should_split"]:
            self.decomposition_stats["executed_directly"] += 1
            return self._execute_atomic_task(task)
        
        # Krok 2: Dekompozycja zadania
        num_subtasks = complexity_analysis["num_subtasks"]
        # === COMPLEXITY FACTOR: Hard cap na 5 podzada≈Ñ (Heurystyka Ciƒôcia nr 2) ===
        num_subtasks = min(num_subtasks, 5)
        subtask_descriptions = self.coordinator.decompose_task(task, num_subtasks, self.task_manager)
        
        if not subtask_descriptions:
            # Coordinator nie stworzy≈Ç podzada≈Ñ - wykonaj bezpo≈õrednio
            self.decomposition_stats["executed_directly"] += 1
            return self._execute_atomic_task(task)
        
        # Krok 3: Detekcja i eliminacja duplikat√≥w
        subtask_descriptions = self.duplication_detector.detect_and_eliminate_duplicates(
            subtask_descriptions, task
        )
        
        if not subtask_descriptions:
            # Po eliminacji duplikat√≥w nie zosta≈Ço nic - wykonaj zadanie bezpo≈õrednio
            self.decomposition_stats["executed_directly"] += 1
            return self._execute_atomic_task(task)
        
        # === SEMANTIC LOOP DETECTION (Heurystyka Ciƒôcia nr 4) ===
        # Sprawd≈∫ czy kt√≥re≈õ z nowych podzada≈Ñ ma semantycznƒÖ pƒôtlƒô z przodkami
        loop_check = self.semantic_loop_detector.detect_semantic_loops(task, self.task_manager)
        
        if loop_check["loop_detected"]:
            self.log(f"‚ö† PƒòTLA BIUROKRATYCZNA WYKRYTA: Zadanie powtarza siƒô - scalanie", Fore.RED)
            self.log(f"  Merge: Level {loop_check['ancestor_match'].level} <- Level {task.level}", Fore.YELLOW)
            # Zamiast dalej dzieliƒá, wykonaj zadanie jako atomic
            self.decomposition_stats["executed_directly"] += 1
            return self._execute_atomic_task(task)
        
        # Utw√≥rz podzadania
        self.decomposition_stats["decomposed"] += 1
        self.task_manager.update_task_status(task.id, TaskStatus.DECOMPOSED)
        
        for idx, subtask_desc in enumerate(subtask_descriptions, 1):
            subtask = self.task_manager.create_task(
                description=subtask_desc,
                task_type=TaskType.SUBTASK,
                level=task.level + 1,
                parent_id=task.id
            )
            self.log(f"Utworzono podzadanie {idx}/{len(subtask_descriptions)}: {subtask.id}", Fore.CYAN)
        
        # Rekursywnie przetw√≥rz wszystkie podzadania
        all_success = True
        for subtask in task.subtasks:
            success = self.process_task_recursive(subtask)
            if success:
                # Zapisz wynik do kontekstu
                self.context_store[subtask.id] = subtask.result
            all_success = all_success and success
        
        if all_success:
            # Agreguj wyniki podzada≈Ñ
            task.result = self._aggregate_subtask_results(task)
            self.task_manager.update_task_status(task.id, TaskStatus.COMPLETED)
            
            # Weryfikacja
            verification = self.verifier.verify_task(task)
            self.task_manager.update_verification(task.id, verification)
            
            if verification["passed"]:
                self.task_manager.update_task_status(task.id, TaskStatus.VERIFIED)
                return True
        
        self.task_manager.update_task_status(task.id, TaskStatus.FAILED)
        return False
    
    def print_statistics(self):
        """Wy≈õwietla statystyki dekompozycji"""
        stats = self.decomposition_stats
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}STATYSTYKI DEKOMPOZYCJI")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}Wszystkich zada≈Ñ: {stats['total_tasks']}")
        print(f"Podzielonych na podzadania: {stats['decomposed']}")
        print(f"Wykonanych bezpo≈õrednio: {stats['executed_directly']}")
        print(f"Maksymalny poziom zagnie≈ºd≈ºenia: {stats['max_level_reached']}")
        print(f"≈örednia z≈Ço≈ºono≈õƒá: {stats['decomposed'] / max(stats['total_tasks'], 1):.2%} zada≈Ñ wymaga≈Ço podzia≈Çu{Style.RESET_ALL}\n")
    
    def save_results(self, task: Task):
        """Zapisuje wszystkie rezultaty do plik√≥w"""
        execution_time = time.time() - self.execution_start_time
        
        self.log(f"\n{Fore.CYAN}{'='*80}", Fore.WHITE)
        self.log(f"Zapisujƒô rezultaty do plik√≥w...", Fore.CYAN)
        self.log(f"{'='*80}\n", Fore.WHITE)
        
        # Zapisz czysty output
        if task.result:
            output_path = self.persistence.save_task_output(task.id, task.result)
            self.log(f"‚úì Czysty wynik: {output_path}", Fore.GREEN)
        
        # Zapisz szczeg√≥≈Çowy raport
        report_path = self.persistence.save_detailed_report(
            task, self.task_manager, self.decomposition_stats, execution_time
        )
        self.log(f"‚úì Raport szczeg√≥≈Çowy: {report_path}", Fore.GREEN)
        
        # Zapisz tekstowy raport (czytelny dla cz≈Çowieka)
        text_report_path = self.persistence.export_as_text_report(
            task, self.decomposition_stats, execution_time
        )
        self.log(f"‚úì Raport tekstowy: {text_report_path}", Fore.GREEN)
        
        # Zapisz hierarchiƒô zada≈Ñ
        hierarchy_path = self.persistence.save_task_hierarchy(task, self.task_manager)
        self.log(f"‚úì Hierarchia zada≈Ñ: {hierarchy_path}", Fore.GREEN)
        
        # Zapisz statystyki
        stats_path = self.persistence.save_decomposition_stats(
            self.decomposition_stats, task.id
        )
        self.log(f"‚úì Statystyki: {stats_path}", Fore.GREEN)
        
        # Zapisz podsumowanie wykonania
        summary_path = self.persistence.save_execution_summary(
            task.id, task.description, self.decomposition_stats, execution_time
        )
        self.log(f"‚úì Podsumowanie: {summary_path}", Fore.GREEN)
        
        # Wy≈õwietl podsumowanie persistencji
        self.persistence.print_summary()
    
    def log(self, message: str, color=Fore.WHITE):
        """Loguje wiadomo≈õƒá z kolorami"""
        print(f"{color}{message}{Style.RESET_ALL}")
    
    def _execute_atomic_task(self, task: Task) -> bool:
        """Wykonuje zadanie atomowe"""
        self.task_manager.update_task_status(task.id, TaskStatus.IN_PROGRESS)
        
        # Zbierz kontekst z zada≈Ñ na tym samym poziomie
        context = self._gather_context(task)
        
        # Przydziel executora
        executor = self.get_next_executor()
        
        # Wykonaj zadanie
        result = executor.execute_task(task, context)
        self.task_manager.update_task_result(task.id, result)
        self.task_manager.update_task_status(task.id, TaskStatus.COMPLETED)
        
        # Weryfikacja
        verification = self.verifier.verify_task(task)
        self.task_manager.update_verification(task.id, verification)
        
        if verification["passed"]:
            self.task_manager.update_task_status(task.id, TaskStatus.VERIFIED)
            return True
        else:
            self.task_manager.update_task_status(task.id, TaskStatus.FAILED)
            return False
    
    def _aggregate_subtask_results(self, task: Task) -> str:
        """Agreguje wyniki podzada≈Ñ"""
        results = []
        for subtask in task.subtasks:
            if subtask.result:
                results.append(f"[{subtask.id}] {subtask.result}")
        
        aggregated = f"Zadanie '{task.description}' zosta≈Ço uko≈Ñczone poprzez wykonanie {len(task.subtasks)} podzada≈Ñ:\n\n"
        aggregated += "\n\n".join(results)
        
        return aggregated
    
    def _gather_context(self, task: Task) -> Dict[str, Any]:
        """Zbiera kontekst z poprzednich zada≈Ñ"""
        context = {}
        
        # Dodaj kontekst z zadania rodzica
        if task.parent_id:
            parent = self.task_manager.get_task(task.parent_id)
            if parent:
                context["parent_task"] = parent.description
                
                # Dodaj wyniki innych podzada≈Ñ tego samego rodzica
                for sibling in parent.subtasks:
                    if sibling.id != task.id and sibling.result:
                        context[f"sibling_{sibling.id}"] = sibling.result[:200]
        
        return context
